{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "7DxzXi9HZf9V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from collections import Counter\n",
        "import copy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "arZHuGWuZf9X"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "XdRTirSoZ5Ev",
        "outputId": "7ccbdf68-a733-4422-fd61-9b439db93866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-16 17:08:13--  https://www.cse.iitb.ac.in/~pjyothi/cs335/dataset-lab9.tar.gz\n",
            "Resolving www.cse.iitb.ac.in (www.cse.iitb.ac.in)... 103.21.127.134\n",
            "Connecting to www.cse.iitb.ac.in (www.cse.iitb.ac.in)|103.21.127.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 425206 (415K) [application/x-gzip]\n",
            "Saving to: ‘dataset-lab9.tar.gz’\n",
            "\n",
            "dataset-lab9.tar.gz 100%[===================>] 415.24K   495KB/s    in 0.8s    \n",
            "\n",
            "2023-10-16 17:08:15 (495 KB/s) - ‘dataset-lab9.tar.gz’ saved [425206/425206]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.cse.iitb.ac.in/~pjyothi/cs335/dataset-lab9.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "LLDAGNADg2N4"
      },
      "outputs": [],
      "source": [
        "!mv \"dataset-lab9.tar.gz\" dataset.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "b41umwFoj451",
        "outputId": "b5d97941-7fca-437f-aea2-02da7d8fad4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset/\n",
            "dataset/test.csv\n",
            "dataset/validation.txt\n",
            "dataset/train.txt\n"
          ]
        }
      ],
      "source": [
        "!tar -xvzf dataset.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t-TSgyinQZw"
      },
      "source": [
        "## Training character-based LSTM language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "FuHtN6rgZf9X",
        "outputId": "6a0b1e18-8ebf-49c6-da98-39e0e83c44e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1016242\n",
            "51726\n"
          ]
        }
      ],
      "source": [
        "# load ascii text and convert to lowercase\n",
        "train_file = \"dataset/train.txt\"\n",
        "train_raw_text = open(train_file, 'r', encoding='utf-8').read()\n",
        "train_raw_text = train_raw_text.lower()\n",
        "print(len(train_raw_text))\n",
        "\n",
        "val_file = \"dataset/validation.txt\"\n",
        "val_raw_text = open(val_file, 'r', encoding='utf-8').read()\n",
        "val_raw_text = val_raw_text.lower()\n",
        "print(len(val_raw_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "uMJu6QdHncD7",
        "outputId": "b486554e-9d2c-4394-b1ac-aa632067f0e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150000 30000\n"
          ]
        }
      ],
      "source": [
        "# extract unique chars\n",
        "train_chars = [char for char in train_raw_text[0:150000]]  # taking a subset to enable faster training times\n",
        "val_chars = [char for char in val_raw_text[0:30000]]\n",
        "train_chars_uniq = list(set(train_chars))\n",
        "train_chars_uniq.append(\"[UNK]\")\n",
        "\n",
        "print(len(train_chars), len(val_chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "6481Z342ne2c",
        "outputId": "c55c9ca9-5642-433f-f62b-f446dd4ad342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'[UNK]': 0, 'f': 1, 'i': 2, 'r': 3, 's': 4, 't': 5, ' ': 6, 'c': 7, 'z': 8, 'e': 9, 'n': 10, ':': 11, '\\n': 12, 'b': 13, 'o': 14, 'w': 15, 'p': 16, 'd': 17, 'a': 18, 'y': 19, 'u': 20, 'h': 21, ',': 22, 'm': 23, 'k': 24, '.': 25, 'l': 26, 'v': 27, '?': 28, \"'\": 29, 'g': 30, ';': 31, '!': 32, 'j': 33, '-': 34, 'q': 35, 'x': 36, '&': 37}\n"
          ]
        }
      ],
      "source": [
        "# create mapping of unique chars to integers\n",
        "char_to_int = {}\n",
        "char_to_int[\"[UNK]\"] = 0\n",
        "cnt = 1\n",
        "for char in train_chars:\n",
        "  if char in train_chars_uniq and char not in char_to_int:\n",
        "    char_to_int[char] = cnt\n",
        "    cnt += 1\n",
        "print(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Zj_jjQF_nxjv",
        "outputId": "550cb80e-a252-47c2-d00d-c04659dcb493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train chars:  150000\n",
            "Total val chars:  30000\n",
            "Total char vocab size:  38\n"
          ]
        }
      ],
      "source": [
        "\n",
        "n_train_chars = len(train_chars)\n",
        "n_val_chars = len(val_chars)\n",
        "\n",
        "n_vocab = len(train_chars_uniq)\n",
        "print(\"Total train chars: \", n_train_chars)\n",
        "print(\"Total val chars: \", n_val_chars)\n",
        "print(\"Total char vocab size: \", n_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "GFuJHpkIoWz3",
        "outputId": "df1c2473-9245-4d67-f8a1-627d67eb833c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of train patterns:  149900\n",
            "first citizen:\n",
            "before we proceed any further, hear me speak.\n",
            "\n",
            "all:\n",
            "speak, speak.\n",
            "\n",
            "first citizen:\n",
            "you\n"
          ]
        }
      ],
      "source": [
        "# Prepare the training dataset of input to output pairs encoded as integers\n",
        "seq_length_char = 100\n",
        "train_dataX = []\n",
        "train_dataY = []\n",
        "\n",
        "for i in range(0, n_train_chars - seq_length_char, 1):\n",
        "    seq_in = train_raw_text[i:i + seq_length_char]\n",
        "    seq_out = train_raw_text[i + seq_length_char]\n",
        "\n",
        "    if seq_out not in char_to_int: continue\n",
        "\n",
        "    train_dataX.append([char_to_int[char] if char in char_to_int else char_to_int[\"[UNK]\"] for char in seq_in])\n",
        "    train_dataY.append(char_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(train_dataX)\n",
        "print(\"Total number of train patterns: \", n_patterns)\n",
        "\n",
        "example = [int_to_char[char] for char in train_dataX[0]]\n",
        "print(\"\".join(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "GCrshisdpLC8",
        "outputId": "8612ed8b-6755-4493-9900-f5992672934f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of validation patterns:  29900\n",
            "she vied so fast, protesting oath on oath,\n",
            "that in a twink she won me to her love.\n",
            "o, you are novice\n"
          ]
        }
      ],
      "source": [
        "# Prepare the validation dataset of input to output pairs encoded as integers\n",
        "val_dataX = []\n",
        "val_dataY = []\n",
        "\n",
        "for i in range(0, n_val_chars - seq_length_char, 1):\n",
        "    seq_in = val_raw_text[i:i + seq_length_char]\n",
        "    seq_out = val_raw_text[i+seq_length_char]\n",
        "    assert len(seq_in) == seq_length_char\n",
        "    if seq_out not in char_to_int: continue\n",
        "\n",
        "    val_dataX.append([char_to_int[char] if char in char_to_int else char_to_int[\"[UNK]\"] for char in seq_in])\n",
        "    val_dataY.append(char_to_int[seq_out])\n",
        "\n",
        "n_val_patterns = len(val_dataX)\n",
        "print(\"Total number of validation patterns: \", n_val_patterns)\n",
        "example = [int_to_char[char] for char in val_dataX[0]]\n",
        "print(\"\".join(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "dE8VaOnqxh-l",
        "outputId": "274db1a6-e207-4d5e-e2fb-ef3eb595ce8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179800 179800\n",
            "161820 17980\n"
          ]
        }
      ],
      "source": [
        "# Randomizing the train and val sentences for better learning\n",
        "\n",
        "all_X = train_dataX + val_dataX\n",
        "all_Y = train_dataY + val_dataY\n",
        "print(len(all_X), len(all_Y))\n",
        "\n",
        "indexes = [i for i in range(len(all_X))]\n",
        "random.seed(42)\n",
        "random.shuffle(indexes)\n",
        "\n",
        "train_X = [all_X[i] for i in indexes[0:int(0.9*len(indexes))]]\n",
        "train_Y = [all_Y[i] for i in indexes[0:int(0.9*len(indexes))]]\n",
        "\n",
        "val_X = [all_X[i] for i in indexes[int(0.9*len(indexes)):]]\n",
        "val_Y = [all_Y[i] for i in indexes[int(0.9*len(indexes)):]]\n",
        "\n",
        "print(len(train_X), len(val_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "uAQs3pdwpVIV",
        "outputId": "07a6bc0e-8184-461c-88f4-0b2f98700ebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([161820, 100]) torch.Size([161820])\n",
            "torch.Size([17980, 100]) torch.Size([17980])\n"
          ]
        }
      ],
      "source": [
        "X_train, Y_train = torch.tensor(train_X, dtype=torch.int32), torch.tensor(train_Y)\n",
        "X_val, Y_val = torch.tensor(val_X, dtype=torch.int32), torch.tensor(val_Y)\n",
        "\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_val.shape, Y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "gtrMSW9YrpGZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "vectorized_train_dataset = TensorDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(vectorized_train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "vectorized_val_dataset = TensorDataset(X_val, Y_val)\n",
        "val_loader = DataLoader(vectorized_val_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "UXcUglRopko2",
        "outputId": "a1dc860d-b958-4a08-8a26-2dd25f8c45f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "OtG3YtACpoz5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMTextGeneratorChar(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_len, n_layers, hidden_dim):\n",
        "    #     super(LSTMTextGeneratorChar, self).__init__()\n",
        "    #     # TODO: Create an LSTM LM followed by a linear layer with dropout (p = 0.3)\n",
        "    #     # n_vocab: vocabulary size\n",
        "    #     # embed_len: dimensionality of the input embeddings\n",
        "    #     # n_layers: number of LSTM layers\n",
        "    #     # hidden_dim: dimensionality of the LSTM hidden states\n",
        "    #     self.n_vocab=n_vocab\n",
        "    #     self.embed_len=embed_len\n",
        "    #     self.n_layers=n_layers\n",
        "    #     self.hidden_dim=hidden_dim\n",
        "\n",
        "\n",
        "\n",
        "    #     # self.word_embedding = nn.Embedding(n_vocab, embed_len)\n",
        "\n",
        "    #     lstm = nn.LSTM(embed_len, hidden_dim, n_layers, batch_first=True)\n",
        "\n",
        "    #     linear=nn.Linear(hidden_dim,n_vocab)\n",
        "    #     dropout=nn.Dropout(0.3)\n",
        "        super(LSTMTextGeneratorChar, self).__init__()\n",
        "\n",
        "        self.n_vocab = n_vocab\n",
        "        self.embed_len = embed_len\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Define the word embedding layer\n",
        "        self.word_embedding = nn.Embedding(n_vocab, embed_len)\n",
        "\n",
        "        # Define the LSTM layer\n",
        "        self.lstm = nn.LSTM(embed_len, hidden_dim, n_layers, batch_first=True)\n",
        "\n",
        "        # Define the linear layer followed by dropout\n",
        "        self.linear = nn.Linear(hidden_dim, n_vocab)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        # TODO: Make sure you go through and understand all the following lines of code\n",
        "        embeddings = self.word_embedding(X_batch)\n",
        "\n",
        "\n",
        "        hidden, carry = torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(device), torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(device)\n",
        "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
        "        return self.linear(self.dropout(output[:,-1, :]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "2k-ATigepu5X"
      },
      "outputs": [],
      "source": [
        "def train(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
        "  set_seed(42)\n",
        "\n",
        "  best_checkpoint = None\n",
        "  val_prev_loss = 10000000.0\n",
        "\n",
        "  for i in range(1, epochs+1):\n",
        "      losses = []\n",
        "      print(\"Current epoch: \", i)\n",
        "      model.train()\n",
        "\n",
        "      for X, Y in tqdm(train_loader):\n",
        "        Y_preds = model(X.to(device))\n",
        "\n",
        "        loss = loss_fn(Y_preds, Y.to(device))\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      if (i%3) == 0:\n",
        "          val_losses = []\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "            for X, Y in tqdm(val_loader):\n",
        "              # TODO: Complete the implementation and populate val_losses\n",
        "              Y_val_preds = model(X.to(device))\n",
        "              val_loss = loss_fn(Y_val_preds, Y.to(device))\n",
        "              val_losses.append(val_loss.item())\n",
        "\n",
        "          training_perplexity = torch.exp(torch.tensor(losses).mean()).item()\n",
        "          validation_perplexity = torch.exp(torch.tensor(val_losses).mean()).item()\n",
        "          print(f\"Training Perplexity: {training_perplexity:.2f}\")\n",
        "          print(f\"Validation Perplexity: {validation_perplexity:.2f}\")\n",
        "\n",
        "\n",
        "          # TODO: Print training/validation perplexities\n",
        "\n",
        "          if torch.tensor(val_losses).mean().item() < val_prev_loss:\n",
        "            print(\"checkpointing current model\")\n",
        "            best_checkpoint = copy.deepcopy(model)\n",
        "            val_prev_loss = torch.tensor(val_losses).mean().item()\n",
        "\n",
        "  return best_checkpoint, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "z_zO7OHlpzmx",
        "outputId": "4a4f2e8c-61dd-4e5b-f316-eb36f203edc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.64 ms, sys: 0 ns, total: 4.64 ms\n",
            "Wall time: 4.51 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "epochs = 30\n",
        "learning_rate = 5e-3\n",
        "embed_len = 100\n",
        "hidden_dim = 128\n",
        "n_layers=1\n",
        "\n",
        "set_seed(42)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "text_generator = LSTMTextGeneratorChar(n_vocab, embed_len, n_layers, hidden_dim).to(device)\n",
        "optimizer = Adam(text_generator.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "BGBvKpKGp8V1",
        "outputId": "e091ef4b-8b03-42ae-a2b7-3d86141601fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 124.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:09<00:00, 130.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:09<00:00, 126.74it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 280.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 5.47\n",
            "Validation Perplexity: 4.98\n",
            "checkpointing current model\n",
            "Current epoch:  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 125.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 125.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 124.70it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 279.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 5.03\n",
            "Validation Perplexity: 4.70\n",
            "checkpointing current model\n",
            "Current epoch:  7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 126.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 124.97it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 281.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 4.85\n",
            "Validation Perplexity: 4.63\n",
            "checkpointing current model\n",
            "Current epoch:  10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.37it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 250.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 4.77\n",
            "Validation Perplexity: 4.60\n",
            "checkpointing current model\n",
            "Current epoch:  13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:09<00:00, 127.40it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 231.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 4.72\n",
            "Validation Perplexity: 4.57\n",
            "checkpointing current model\n",
            "Current epoch:  16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 120.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.80it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 284.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 4.70\n",
            "Validation Perplexity: 4.57\n",
            "Current epoch:  19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 124.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 125.93it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 214.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 4.69\n",
            "Validation Perplexity: 4.58\n",
            "Current epoch:  22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:09<00:00, 127.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.32it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 277.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 4.70\n",
            "Validation Perplexity: 4.54\n",
            "checkpointing current model\n",
            "Current epoch:  25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 124.30it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 276.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 4.73\n",
            "Validation Perplexity: 4.58\n",
            "Current epoch:  28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 123.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1265/1265 [00:10<00:00, 119.37it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 281.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 4.77\n",
            "Validation Perplexity: 4.54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "best_checkpoint_char, last_checkpoint_char = train(text_generator, loss_fn, optimizer, train_loader, val_loader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt8QAwv9xzmw"
      },
      "source": [
        "## Training word-based LSTM language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "FW7lTFtwx51B",
        "outputId": "52090496-96c0-48f5-a29d-6082038da1d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "155159 7833\n"
          ]
        }
      ],
      "source": [
        "# extract unique words\n",
        "train_words = [word for word in train_raw_text.split(\" \")]\n",
        "val_words = [word for word in val_raw_text.split(\" \")]\n",
        "\n",
        "# make a list of train words\n",
        "train_words_subset_vocab = list(set(train_words[0:int(0.8*len(train_words))]))\n",
        "train_words_subset_vocab.append(\"[UNK]\")\n",
        "train_words_vocab = list(set(train_words))\n",
        "train_words_subset_vocab_set = set(train_words_subset_vocab)\n",
        "\n",
        "print(len(train_words), len(val_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ZmTM2oEiyHke"
      },
      "outputs": [],
      "source": [
        "# create mapping of unique words to integers\n",
        "word_to_int = {}\n",
        "word_to_int[\"[UNK]\"] = 0\n",
        "cnt = 1\n",
        "for word in train_words:\n",
        "  if word in train_words_subset_vocab_set and word not in word_to_int:\n",
        "    word_to_int[word] = cnt\n",
        "    cnt += 1\n",
        "\n",
        "int_to_word = dict((i, w) for w, i in word_to_int.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Nzsor-11yK0Z",
        "outputId": "98c637f2-392f-45d0-a6c9-9c29acdc68be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train words:  155159\n",
            "Total val words:  7833\n",
            "Total vocab size:  31954\n"
          ]
        }
      ],
      "source": [
        "n_train_words = len(train_words)\n",
        "n_val_words = len(val_words)\n",
        "\n",
        "n_vocab = len(train_words_subset_vocab)\n",
        "print(\"Total train words: \", n_train_words)\n",
        "print(\"Total val words: \", n_val_words)\n",
        "print(\"Total vocab size: \", n_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "dLiuGNzXyNQI",
        "outputId": "a86fbba8-7f27-48bd-cb3f-ed8b326b03ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train Patterns:  142965\n",
            "first [UNK] before we proceed any further, hear me speak. all: speak, speak. first [UNK] you are all resolved rather to die than to [UNK] all: [UNK] [UNK] first [UNK]\n",
            "[UNK] before we proceed any further, hear me speak. all: speak, speak. first [UNK] you are all resolved rather to die than to [UNK] all: [UNK] [UNK] first [UNK] first,\n"
          ]
        }
      ],
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 30\n",
        "train_dataX = []\n",
        "train_dataY = []\n",
        "\n",
        "train_raw_words = train_raw_text.split()\n",
        "\n",
        "for i in range(0, n_train_words - seq_length, 1):\n",
        "    seq_in = train_raw_words[i:i + seq_length]\n",
        "    seq_out = train_raw_words[i+seq_length]\n",
        "    assert len(seq_in) == seq_length\n",
        "\n",
        "    if seq_out not in word_to_int: continue\n",
        "\n",
        "    train_dataX.append([word_to_int[word] if word in word_to_int else word_to_int[\"[UNK]\"] for word in seq_in])\n",
        "    train_dataY.append(word_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(train_dataX)\n",
        "print(\"Total train Patterns: \", n_patterns)\n",
        "example = [int_to_word[word] for word in train_dataX[0]]\n",
        "print(\" \".join(example))\n",
        "\n",
        "example = [int_to_word[word] for word in train_dataX[1]]\n",
        "print(\" \".join(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "zZqCfCGsyPr2",
        "outputId": "ac08d9f4-19d0-43f0-c804-9e4d75aab99d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total val Patterns:  6306\n",
            "she [UNK] so fast, [UNK] oath on oath, that in a [UNK] she won me to her love. o, you are [UNK] 'tis a world to see, how [UNK] when\n"
          ]
        }
      ],
      "source": [
        "# prepare the validation dataset of input to output pairs encoded as integers\n",
        "val_dataX = []\n",
        "val_dataY = []\n",
        "\n",
        "val_raw_words = val_raw_text.split()\n",
        "\n",
        "for i in range(0, n_val_words - seq_length, 1):\n",
        "    seq_in = val_raw_words[i:i + seq_length]\n",
        "    #print(\" \".join(seq_in))\n",
        "    seq_out = val_raw_words[i+seq_length]\n",
        "    assert len(seq_in) == seq_length\n",
        "    if seq_out not in word_to_int: continue\n",
        "\n",
        "    val_dataX.append([word_to_int[word] if word in word_to_int else word_to_int[\"[UNK]\"] for word in seq_in])\n",
        "    val_dataY.append(word_to_int[seq_out])\n",
        "\n",
        "n_val_patterns = len(val_dataX)\n",
        "print(\"Total val Patterns: \", n_val_patterns)\n",
        "example = [int_to_word[word] for word in val_dataX[0]]\n",
        "print(\" \".join(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Yotrhqc7iidi",
        "outputId": "717ee021-4a0a-4340-bb51-a031152a3d37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149271 149271\n",
            "141807 7464\n"
          ]
        }
      ],
      "source": [
        "# randomizing the train and val sentences for better learning\n",
        "all_X = train_dataX + val_dataX\n",
        "all_Y = train_dataY + val_dataY\n",
        "print(len(all_X), len(all_Y))\n",
        "\n",
        "indexes = [i for i in range(len(all_X))]\n",
        "random.seed(42)\n",
        "random.shuffle(indexes)\n",
        "\n",
        "train_X = [all_X[i] for i in indexes[0:int(0.95*len(indexes))]]\n",
        "train_Y = [all_Y[i] for i in indexes[0:int(0.95*len(indexes))]]\n",
        "\n",
        "val_X = [all_X[i] for i in indexes[int(0.95*len(indexes)):]]\n",
        "val_Y = [all_Y[i] for i in indexes[int(0.95*len(indexes)):]]\n",
        "\n",
        "print(len(train_X), len(val_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "wx1DwS5KySZw",
        "outputId": "8ccae17a-1b14-4b3f-d426-6a7b034354b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([141807, 30]) torch.Size([141807])\n",
            "torch.Size([7464, 30]) torch.Size([7464])\n"
          ]
        }
      ],
      "source": [
        "X_train, Y_train = torch.tensor(train_X, dtype=torch.int32), torch.tensor(train_Y)\n",
        "X_val, Y_val = torch.tensor(val_X, dtype=torch.int32), torch.tensor(val_Y)\n",
        "\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_val.shape, Y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "V7IwA2i2yWhr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "vectorized_train_dataset = TensorDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(vectorized_train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "vectorized_val_dataset = TensorDataset(X_val, Y_val)\n",
        "val_loader = DataLoader(vectorized_val_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "c1AGcAOMZf9Z",
        "outputId": "12743992-8120-4f0c-c79a-f7848f87580b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "yWWHicPVZf9Z"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMTextGeneratorWord(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_len, n_layers, hidden_dim):\n",
        "        # TODO: Complete the __init__ definition (as in char-based LSTMs)\n",
        "        super(LSTMTextGeneratorWord, self).__init__()\n",
        "\n",
        "        self.n_vocab = n_vocab\n",
        "        self.embed_len = embed_len\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Define the word embedding layer\n",
        "        self.word_embedding = nn.Embedding(n_vocab, embed_len)\n",
        "\n",
        "        # Define the LSTM layer\n",
        "        self.lstm = nn.LSTM(embed_len, hidden_dim, n_layers, batch_first=True)\n",
        "\n",
        "        # Define the linear layer followed by dropout\n",
        "        self.linear = nn.Linear(hidden_dim, n_vocab)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.word_embedding(X_batch)\n",
        "\n",
        "        hidden, carry = torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(device), torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(device)\n",
        "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
        "        # output, (hidden, carry) = self.lstm(embeddings)\n",
        "        return self.linear(self.dropout(output[:,-1, :]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Mv6Zxx13Zf9a",
        "outputId": "0f6e947d-3ff9-4483-bf78-5e90af72dda6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 146 ms, sys: 33 ms, total: 179 ms\n",
            "Wall time: 184 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 5e-3\n",
        "embed_len = 300\n",
        "hidden_dim = 256\n",
        "n_layers=2\n",
        "\n",
        "set_seed(42)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "text_generator = LSTMTextGeneratorWord(n_vocab, embed_len, n_layers, hidden_dim).to(device)\n",
        "optimizer = Adam(text_generator.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "YedKC3DB385O",
        "outputId": "fcf133eb-4079-4113-e19e-387aabe71d7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1108/1108 [00:21<00:00, 51.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1108/1108 [00:21<00:00, 51.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1108/1108 [00:21<00:00, 50.48it/s]\n",
            "100%|██████████| 59/59 [00:00<00:00, 170.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 630.42\n",
            "Validation Perplexity: 714.66\n",
            "checkpointing current model\n",
            "Current epoch:  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1108/1108 [00:21<00:00, 50.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1108/1108 [00:21<00:00, 50.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1108/1108 [00:21<00:00, 50.81it/s]\n",
            "100%|██████████| 59/59 [00:00<00:00, 168.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 360.99\n",
            "Validation Perplexity: 710.95\n",
            "checkpointing current model\n",
            "Current epoch:  7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1108/1108 [00:21<00:00, 50.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1108/1108 [00:21<00:00, 50.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch:  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1108/1108 [00:21<00:00, 50.80it/s]\n",
            "100%|██████████| 59/59 [00:00<00:00, 135.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perplexity: 227.81\n",
            "Validation Perplexity: 891.45\n",
            "Current epoch:  10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1108/1108 [00:21<00:00, 50.77it/s]\n"
          ]
        }
      ],
      "source": [
        "best_checkpoint_word, last_checkpoint_word = train(text_generator, loss_fn, optimizer, train_loader, val_loader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob4DKbrUiMos"
      },
      "source": [
        "## Generating text starting from a prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Q8ZbkJ4RZf9a",
        "outputId": "397509f5-7c7c-49ac-e4a3-9aa96b77db06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt is:  bride and bridegroom coming home? gremio: a bridegroom say you? 'tis a groom indeed, a grumbling groom, and that the girl shall find. tranio: curster than she? why, 'tis impossible.\n",
            "Generation is:\n",
            "\n",
            "and i am a man of the house of the "
          ]
        }
      ],
      "source": [
        "np.random.seed(48)\n",
        "start = np.random.randint(0, len(val_raw_words)-seq_length)\n",
        "prompt = val_raw_words[start:start+seq_length]\n",
        "print(\"Prompt is: \", \" \".join(prompt))\n",
        "pattern = [word_to_int[w] if w in word_to_int else word_to_int[\"[UNK]\"] for w in prompt]\n",
        "\n",
        "last_checkpoint_word.eval()\n",
        "set_seed(54)\n",
        "print(\"Generation is:\")\n",
        "print()\n",
        "with torch.no_grad():\n",
        "    for i in range(10):\n",
        "        # TODO: Generate the next ten words starting from prompt\n",
        "        # Convert the pattern to a tensor\n",
        "        pattern_tensor = torch.tensor(pattern, dtype=torch.int32).unsqueeze(0).to(device)\n",
        "\n",
        "        # Use the model to generate the next word\n",
        "        next_word_logits = last_checkpoint_word(pattern_tensor)\n",
        "\n",
        "        # Apply a softmax function to get probabilities\n",
        "        next_word_index = torch.argmax(next_word_logits,dim=-1).item()\n",
        "\n",
        "        # Convert the word index back to a word\n",
        "        next_word = int_to_word[next_word_index]\n",
        "\n",
        "        # Print the generated word\n",
        "        print(next_word, end=\" \")\n",
        "        # Update the pattern by adding the generated word's integer index\n",
        "        pattern.append(next_word_index)\n",
        "\n",
        "        # Remove the first word in the pattern to keep the length constant\n",
        "        pattern = pattern[1:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cZdLTNciX8u"
      },
      "source": [
        "## Creating the submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "aB4GdeWaZf9a",
        "outputId": "40e4a309-30d7-4448-87ce-98a67433be04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 0.06877771019935608], ['and', 0.03624435141682625], ['my', 0.0319378636777401], ['ay,', 0.029673200100660324], ['what', 0.028927214443683624], ['the', 0.028812628239393234], ['why,', 0.025021085515618324], ['well,', 0.02181084267795086], ['o', 0.020405389368534088], ['a', 0.01868399977684021], ['he', 0.01843300089240074], ['o,', 0.01839754916727543], ['you', 0.01837000623345375], ['no,', 0.01834946870803833], [\"'tis\", 0.015607225708663464], ['nay,', 0.012782913632690907], ['king', 0.012461908161640167], ['but', 0.012355445884168148], ['how', 0.012009838595986366], ['what,', 0.01183494832366705], ['come,', 0.011769074015319347], ['if', 0.010838332585990429], ['good', 0.010691329836845398], ['not', 0.010589848272502422], ['so', 0.010029594413936138], ['for', 0.009642292745411396], ['it', 0.009501303546130657], ['in', 0.009496590122580528], ['then', 0.009235513396561146], ['we', 0.009130850434303284], ['with', 0.009009452536702156], ['now,', 0.00851100217550993], ['that', 0.008453532122075558], ['of', 0.008440759032964706], ['by', 0.008167875930666924], ['to', 0.008138797245919704], ['sir,', 0.008045229129493237], ['as', 0.008014105260372162], ['this', 0.007975426502525806], [\"i'll\", 0.007641502656042576], ['thou', 0.007639715913683176], ['who', 0.00694329384714365], ['now', 0.006904652342200279], ['why', 0.006871873512864113], ['let', 0.006703007500618696], ['your', 0.006251564249396324], ['go,', 0.006044916342943907], ['madam,', 0.005727254319936037], ['marry,', 0.005683979019522667], ['go', 0.00547401886433363], ['no', 0.0054705142974853516], [\"that's\", 0.005305132828652859], ['indeed,', 0.0046175685711205006], ['she', 0.004582236520946026], [\"let's\", 0.004491773899644613], ['thy', 0.004322289954870939], ['here', 0.004310037940740585], ['have', 0.003958710003644228], ['they', 0.003911456558853388], ['at', 0.0038093714974820614], ['where', 0.003597867675125599], ['shall', 0.0033935278188437223], ['god', 0.0033624551724642515], ['there', 0.0033341795206069946], ['sweet', 0.0031080394983291626], ['when', 0.0030133300460875034], ['do', 0.0029948113951832056], ['say,', 0.0029698589351028204], ['yet', 0.002965510357171297], ['take', 0.0029525435529649258], ['all', 0.0029083702247589827], ['his', 0.0028886031359434128], ['alas,', 0.0027652729768306017], ['give', 0.0025411141104996204], ['is', 0.002455008216202259], ['here,', 0.0023255511187016964], ['one', 0.002271367935463786], ['be', 0.0022587350104004145], ['well', 0.002195467008277774], ['even', 0.0021720267832279205], ['yes,', 0.0020682376343756914], ['farewell,', 0.0020471273455768824], ['ah,', 0.0020183708984404802], ['will', 0.0019789463840425014], ['tell', 0.0019743209704756737], ['so,', 0.0019335380056872964], ['had', 0.0018524018814787269], [\"here's\", 0.0018348541343584657], ['nor', 0.0018094711704179645], ['and,', 0.0017430256120860577], ['yea,', 0.0017371636349707842], ['worthy', 0.0016449608374387026], ['on', 0.0015408105682581663], ['very', 0.0015101643512025476], ['fie,', 0.001497326884418726], ['pray', 0.0014894524356350303], ['then,', 0.0014712625415995717], ['come', 0.0014536542585119605], ['thus', 0.001447563525289297], ['has', 0.001413965248502791]]\n",
            "(200, 100, 2)\n"
          ]
        }
      ],
      "source": [
        "# create the submission file\n",
        "df = pd.read_csv(\"dataset/test.csv\")\n",
        "ids, sents = list(df[\"id\"]), list(df[\"inputs\"])\n",
        "outputs = []\n",
        "\n",
        "# making the predictions\n",
        "last_checkpoint_word.eval()\n",
        "softmax = nn.Softmax()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sent in sents:\n",
        "        # format input array of int into PyTorch tensor\n",
        "        sent_ids = [word_to_int[word] if word in word_to_int else word_to_int[\"[UNK]\"] for word in sent.split()]\n",
        "        x = torch.tensor(sent_ids, dtype=torch.int32).reshape(1, len(sent_ids))\n",
        "        x = torch.tensor(x, dtype=torch.int32).detach()\n",
        "        # generate logits as output from the model\n",
        "        prediction = last_checkpoint_word(x.to(device))[0]\n",
        "        # take softmax for probs\n",
        "        # TODO: shape of outputs is (200, 100, 2)\n",
        "        # For each of the 200 test sentences in test.csv, given the prefix in sent, outputs contains\n",
        "        # the list of top 100 next-word predictions and its corresponding probabilities\n",
        "        # Apply softmax to get probabilities\n",
        "        prob = softmax(prediction)\n",
        "\n",
        "        # Find the top 100 words with the highest probabilities\n",
        "        top_words = torch.argsort(prob, descending=True)[:100]\n",
        "        top_probabilities = prob[top_words].cpu().numpy()\n",
        "\n",
        "\n",
        "        # Store the top words and their corresponding probabilities for this sentence\n",
        "        X=[[int_to_word[w.item()],p.item()]for w,p in zip(top_words,top_probabilities)]\n",
        "        outputs.append(X)\n",
        "\n",
        "\n",
        "print(outputs[0])\n",
        "print(np.shape(outputs))\n",
        "\n",
        "# save the output file\n",
        "np.save(\"outputs\", outputs)\n",
        "# TODO: Create new cells below for the extra credit part\n",
        "# TODO: Also save the outputs for the extra credit part in a new file, np.save(\"ec-outputs\", outputs)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "4e15b00ded0040948360ef603d66f989a0fbd28a349705db7c3e6d1dfe8940bc"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}