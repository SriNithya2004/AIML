{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DxzXi9HZf9V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from collections import Counter\n",
        "import copy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arZHuGWuZf9X"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdRTirSoZ5Ev"
      },
      "outputs": [],
      "source": [
        "!wget https://www.cse.iitb.ac.in/~pjyothi/cs335/dataset-lab9.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLDAGNADg2N4"
      },
      "outputs": [],
      "source": [
        "!mv \"dataset-lab9.tar.gz\" dataset.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b41umwFoj451"
      },
      "outputs": [],
      "source": [
        "!tar -xvzf dataset.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t-TSgyinQZw"
      },
      "source": [
        "## Training character-based LSTM language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuHtN6rgZf9X"
      },
      "outputs": [],
      "source": [
        "# load ascii text and convert to lowercase\n",
        "train_file = \"dataset/train.txt\"\n",
        "train_raw_text = open(train_file, 'r', encoding='utf-8').read()\n",
        "train_raw_text = train_raw_text.lower()\n",
        "print(len(train_raw_text))\n",
        "\n",
        "val_file = \"dataset/validation.txt\"\n",
        "val_raw_text = open(val_file, 'r', encoding='utf-8').read()\n",
        "val_raw_text = val_raw_text.lower()\n",
        "print(len(val_raw_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMJu6QdHncD7"
      },
      "outputs": [],
      "source": [
        "# extract unique chars\n",
        "train_chars = [char for char in train_raw_text[0:150000]]  # taking a subset to enable faster training times\n",
        "val_chars = [char for char in val_raw_text[0:30000]]\n",
        "train_chars_uniq = list(set(train_chars))\n",
        "train_chars_uniq.append(\"[UNK]\")\n",
        "\n",
        "print(len(train_chars), len(val_chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6481Z342ne2c"
      },
      "outputs": [],
      "source": [
        "# create mapping of unique chars to integers\n",
        "char_to_int = {}\n",
        "char_to_int[\"[UNK]\"] = 0\n",
        "cnt = 1\n",
        "for char in train_chars:\n",
        "  if char in train_chars_uniq and char not in char_to_int:\n",
        "    char_to_int[char] = cnt\n",
        "    cnt += 1\n",
        "print(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj_jjQF_nxjv"
      },
      "outputs": [],
      "source": [
        "n_train_chars = len(train_chars)\n",
        "n_val_chars = len(val_chars)\n",
        "\n",
        "n_vocab = len(train_chars_uniq)\n",
        "print(\"Total train chars: \", n_train_chars)\n",
        "print(\"Total val chars: \", n_val_chars)\n",
        "print(\"Total char vocab size: \", n_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFuJHpkIoWz3"
      },
      "outputs": [],
      "source": [
        "# Prepare the training dataset of input to output pairs encoded as integers\n",
        "seq_length_char = 100\n",
        "train_dataX = []\n",
        "train_dataY = []\n",
        "\n",
        "for i in range(0, n_train_chars - seq_length_char, 1):\n",
        "    seq_in = train_raw_text[i:i + seq_length_char]\n",
        "    seq_out = train_raw_text[i + seq_length_char]\n",
        "\n",
        "    if seq_out not in char_to_int: continue\n",
        "\n",
        "    train_dataX.append([char_to_int[char] if char in char_to_int else char_to_int[\"[UNK]\"] for char in seq_in])\n",
        "    train_dataY.append(char_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(train_dataX)\n",
        "print(\"Total number of train patterns: \", n_patterns)\n",
        "\n",
        "example = [int_to_char[char] for char in train_dataX[0]]\n",
        "print(\"\".join(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCrshisdpLC8"
      },
      "outputs": [],
      "source": [
        "# Prepare the validation dataset of input to output pairs encoded as integers\n",
        "val_dataX = []\n",
        "val_dataY = []\n",
        "\n",
        "for i in range(0, n_val_chars - seq_length_char, 1):\n",
        "    seq_in = val_raw_text[i:i + seq_length_char]\n",
        "    seq_out = val_raw_text[i+seq_length_char]\n",
        "    assert len(seq_in) == seq_length_char\n",
        "    if seq_out not in char_to_int: continue\n",
        "\n",
        "    val_dataX.append([char_to_int[char] if char in char_to_int else char_to_int[\"[UNK]\"] for char in seq_in])\n",
        "    val_dataY.append(char_to_int[seq_out])\n",
        "\n",
        "n_val_patterns = len(val_dataX)\n",
        "print(\"Total number of validation patterns: \", n_val_patterns)\n",
        "example = [int_to_char[char] for char in val_dataX[0]]\n",
        "print(\"\".join(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE8VaOnqxh-l"
      },
      "outputs": [],
      "source": [
        "# Randomizing the train and val sentences for better learning\n",
        "\n",
        "all_X = train_dataX + val_dataX\n",
        "all_Y = train_dataY + val_dataY\n",
        "print(len(all_X), len(all_Y))\n",
        "\n",
        "indexes = [i for i in range(len(all_X))]\n",
        "random.seed(42)\n",
        "random.shuffle(indexes)\n",
        "\n",
        "train_X = [all_X[i] for i in indexes[0:int(0.9*len(indexes))]]\n",
        "train_Y = [all_Y[i] for i in indexes[0:int(0.9*len(indexes))]]\n",
        "\n",
        "val_X = [all_X[i] for i in indexes[int(0.9*len(indexes)):]]\n",
        "val_Y = [all_Y[i] for i in indexes[int(0.9*len(indexes)):]]\n",
        "\n",
        "print(len(train_X), len(val_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAQs3pdwpVIV"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train = torch.tensor(train_X, dtype=torch.int32), torch.tensor(train_Y)\n",
        "X_val, Y_val = torch.tensor(val_X, dtype=torch.int32), torch.tensor(val_Y)\n",
        "\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_val.shape, Y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtrMSW9YrpGZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "vectorized_train_dataset = TensorDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(vectorized_train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "vectorized_val_dataset = TensorDataset(X_val, Y_val)\n",
        "val_loader = DataLoader(vectorized_val_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXcUglRopko2"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtG3YtACpoz5"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMTextGeneratorChar(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_len, n_layers, hidden_dim):\n",
        "        # TODO: Create an LSTM LM followed by a linear layer with dropout (p = 0.3)\n",
        "        # n_vocab: vocabulary size\n",
        "        # embed_len: dimensionality of the input embeddings\n",
        "        # n_layers: number of LSTM layers\n",
        "        # hidden_dim: dimensionality of the LSTM hidden states\n",
        "        pass\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        # TODO: Make sure you go through and understand all the following lines of code\n",
        "        embeddings = self.word_embedding(X_batch)\n",
        "\n",
        "        hidden, carry = torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(device), torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(device)\n",
        "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
        "        return self.linear(self.dropout(output[:,-1, :]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k-ATigepu5X"
      },
      "outputs": [],
      "source": [
        "def train(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
        "  set_seed(42)\n",
        "\n",
        "  best_checkpoint = None\n",
        "  val_prev_loss = 10000000.0\n",
        "\n",
        "  for i in range(1, epochs+1):\n",
        "      losses = []\n",
        "      print(\"Current epoch: \", i)\n",
        "      model.train()\n",
        "\n",
        "      for X, Y in tqdm(train_loader):\n",
        "        Y_preds = model(X.to(device))\n",
        "\n",
        "        loss = loss_fn(Y_preds, Y.to(device))\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      if (i%3) == 0:\n",
        "          val_losses = []\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "            for X, Y in tqdm(val_loader):\n",
        "              # TODO: Complete the implementation and populate val_losses\n",
        "              pass\n",
        "\n",
        "          # TODO: Print training/validation perplexities\n",
        "\n",
        "          if torch.tensor(val_losses).mean().item() < val_prev_loss:\n",
        "            print(\"checkpointing current model\")\n",
        "            best_checkpoint = copy.deepcopy(model)\n",
        "            val_prev_loss = torch.tensor(val_losses).mean().item()\n",
        "\n",
        "  return best_checkpoint, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_zO7OHlpzmx"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "epochs = 30\n",
        "learning_rate = 5e-3\n",
        "embed_len = 100\n",
        "hidden_dim = 128\n",
        "n_layers=1\n",
        "\n",
        "set_seed(42)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "text_generator = LSTMTextGeneratorChar(n_vocab, embed_len, n_layers, hidden_dim).to(device)\n",
        "optimizer = Adam(text_generator.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGBvKpKGp8V1"
      },
      "outputs": [],
      "source": [
        "best_checkpoint_char, last_checkpoint_char = train(text_generator, loss_fn, optimizer, train_loader, val_loader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt8QAwv9xzmw"
      },
      "source": [
        "## Training word-based LSTM language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW7lTFtwx51B"
      },
      "outputs": [],
      "source": [
        "# extract unique words\n",
        "train_words = [word for word in train_raw_text.split(\" \")]\n",
        "val_words = [word for word in val_raw_text.split(\" \")]\n",
        "\n",
        "# make a list of train words\n",
        "train_words_subset_vocab = list(set(train_words[0:int(0.8*len(train_words))]))\n",
        "train_words_subset_vocab.append(\"[UNK]\")\n",
        "train_words_vocab = list(set(train_words))\n",
        "train_words_subset_vocab_set = set(train_words_subset_vocab)\n",
        "\n",
        "print(len(train_words), len(val_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmTM2oEiyHke"
      },
      "outputs": [],
      "source": [
        "# create mapping of unique words to integers\n",
        "word_to_int = {}\n",
        "word_to_int[\"[UNK]\"] = 0\n",
        "cnt = 1\n",
        "for word in train_words:\n",
        "  if word in train_words_subset_vocab_set and word not in word_to_int:\n",
        "    word_to_int[word] = cnt\n",
        "    cnt += 1\n",
        "\n",
        "int_to_word = dict((i, w) for w, i in word_to_int.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzsor-11yK0Z"
      },
      "outputs": [],
      "source": [
        "n_train_words = len(train_words)\n",
        "n_val_words = len(val_words)\n",
        "\n",
        "n_vocab = len(train_words_subset_vocab)\n",
        "print(\"Total train words: \", n_train_words)\n",
        "print(\"Total val words: \", n_val_words)\n",
        "print(\"Total vocab size: \", n_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLiuGNzXyNQI"
      },
      "outputs": [],
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 30\n",
        "train_dataX = []\n",
        "train_dataY = []\n",
        "\n",
        "train_raw_words = train_raw_text.split()\n",
        "\n",
        "for i in range(0, n_train_words - seq_length, 1):\n",
        "    seq_in = train_raw_words[i:i + seq_length]\n",
        "    seq_out = train_raw_words[i+seq_length]\n",
        "    assert len(seq_in) == seq_length\n",
        "\n",
        "    if seq_out not in word_to_int: continue\n",
        "\n",
        "    train_dataX.append([word_to_int[word] if word in word_to_int else word_to_int[\"[UNK]\"] for word in seq_in])\n",
        "    train_dataY.append(word_to_int[seq_out])\n",
        "\n",
        "n_patterns = len(train_dataX)\n",
        "print(\"Total train Patterns: \", n_patterns)\n",
        "example = [int_to_word[word] for word in train_dataX[0]]\n",
        "print(\" \".join(example))\n",
        "\n",
        "example = [int_to_word[word] for word in train_dataX[1]]\n",
        "print(\" \".join(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZqCfCGsyPr2"
      },
      "outputs": [],
      "source": [
        "# prepare the validation dataset of input to output pairs encoded as integers\n",
        "val_dataX = []\n",
        "val_dataY = []\n",
        "\n",
        "val_raw_words = val_raw_text.split()\n",
        "\n",
        "for i in range(0, n_val_words - seq_length, 1):\n",
        "    seq_in = val_raw_words[i:i + seq_length]\n",
        "    #print(\" \".join(seq_in))\n",
        "    seq_out = val_raw_words[i+seq_length]\n",
        "    assert len(seq_in) == seq_length\n",
        "    if seq_out not in word_to_int: continue\n",
        "\n",
        "    val_dataX.append([word_to_int[word] if word in word_to_int else word_to_int[\"[UNK]\"] for word in seq_in])\n",
        "    val_dataY.append(word_to_int[seq_out])\n",
        "\n",
        "n_val_patterns = len(val_dataX)\n",
        "print(\"Total val Patterns: \", n_val_patterns)\n",
        "example = [int_to_word[word] for word in val_dataX[0]]\n",
        "print(\" \".join(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yotrhqc7iidi"
      },
      "outputs": [],
      "source": [
        "# randomizing the train and val sentences for better learning\n",
        "all_X = train_dataX + val_dataX\n",
        "all_Y = train_dataY + val_dataY\n",
        "print(len(all_X), len(all_Y))\n",
        "\n",
        "indexes = [i for i in range(len(all_X))]\n",
        "random.seed(42)\n",
        "random.shuffle(indexes)\n",
        "\n",
        "train_X = [all_X[i] for i in indexes[0:int(0.95*len(indexes))]]\n",
        "train_Y = [all_Y[i] for i in indexes[0:int(0.95*len(indexes))]]\n",
        "\n",
        "val_X = [all_X[i] for i in indexes[int(0.95*len(indexes)):]]\n",
        "val_Y = [all_Y[i] for i in indexes[int(0.95*len(indexes)):]]\n",
        "\n",
        "print(len(train_X), len(val_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx1DwS5KySZw"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train = torch.tensor(train_X, dtype=torch.int32), torch.tensor(train_Y)\n",
        "X_val, Y_val = torch.tensor(val_X, dtype=torch.int32), torch.tensor(val_Y)\n",
        "\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_val.shape, Y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7IwA2i2yWhr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "vectorized_train_dataset = TensorDataset(X_train, Y_train)\n",
        "train_loader = DataLoader(vectorized_train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "vectorized_val_dataset = TensorDataset(X_val, Y_val)\n",
        "val_loader = DataLoader(vectorized_val_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1AGcAOMZf9Z"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWWHicPVZf9Z"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMTextGeneratorWord(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_len, n_layers, hidden_dim):\n",
        "        # TODO: Complete the __init__ definition (as in char-based LSTMs)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.word_embedding(X_batch)\n",
        "\n",
        "        hidden, carry = torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(device), torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(device)\n",
        "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
        "        # output, (hidden, carry) = self.lstm(embeddings)\n",
        "        return self.linear(self.dropout(output[:,-1, :]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv6Zxx13Zf9a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 5e-3\n",
        "embed_len = 300\n",
        "hidden_dim = 256\n",
        "n_layers=2\n",
        "\n",
        "set_seed(42)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "text_generator = LSTMTextGeneratorWord(n_vocab, embed_len, n_layers, hidden_dim).to(device)\n",
        "optimizer = Adam(text_generator.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YedKC3DB385O"
      },
      "outputs": [],
      "source": [
        "best_checkpoint_word, last_checkpoint_word = train(text_generator, loss_fn, optimizer, train_loader, val_loader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob4DKbrUiMos"
      },
      "source": [
        "## Generating text starting from a prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8ZbkJ4RZf9a"
      },
      "outputs": [],
      "source": [
        "np.random.seed(48)\n",
        "start = np.random.randint(0, len(val_raw_words)-seq_length)\n",
        "prompt = val_raw_words[start:start+seq_length]\n",
        "print(\"Prompt is: \", \" \".join(prompt))\n",
        "pattern = [word_to_int[w] if w in word_to_int else word_to_int[\"[UNK]\"] for w in prompt]\n",
        "\n",
        "last_checkpoint_word.eval()\n",
        "set_seed(54)\n",
        "print(\"Generation is:\")\n",
        "print()\n",
        "with torch.no_grad():\n",
        "    for i in range(10):\n",
        "        # TODO: Generate the next ten words starting from prompt\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cZdLTNciX8u"
      },
      "source": [
        "## Creating the submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB4GdeWaZf9a"
      },
      "outputs": [],
      "source": [
        "# create the submission file\n",
        "df = pd.read_csv(\"dataset/test.csv\")\n",
        "ids, sents = list(df[\"id\"]), list(df[\"inputs\"])\n",
        "outputs = []\n",
        "\n",
        "# making the predictions\n",
        "last_checkpoint_word.eval()\n",
        "softmax = nn.Softmax()\n",
        "\n",
        "with torch.no_grad():x\n",
        "    for sent in sents:\n",
        "        # format input array of int into PyTorch tensor\n",
        "        sent_ids = [word_to_int[word] if word in word_to_int else word_to_int[\"[UNK]\"] for word in sent.split()]\n",
        "        x = torch.tensor(sent_ids, dtype=torch.int32).reshape(1, len(sent_ids))\n",
        "        x = torch.tensor(x, dtype=torch.int32).detach()\n",
        "        # generate logits as output from the model\n",
        "        prediction = last_checkpoint_word(x.to(device))[0]\n",
        "        # take softmax for probs\n",
        "        # TODO: shape of outputs is (200, 100, 2)\n",
        "        # For each of the 200 test sentences in test.csv, given the prefix in sent, outputs contains\n",
        "        # the list of top 100 next-word predictions and its corresponding probabilities\n",
        "        pass\n",
        "\n",
        "print(outputs[0])\n",
        "print(np.shape(outputs))\n",
        "\n",
        "# save the output file\n",
        "np.save(\"outputs\", outputs)\n",
        "# TODO: Create new cells below for the extra credit part\n",
        "# TODO: Also save the outputs for the extra credit part in a new file, np.save(\"ec-outputs\", outputs)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "interpreter": {
      "hash": "4e15b00ded0040948360ef603d66f989a0fbd28a349705db7c3e6d1dfe8940bc"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}